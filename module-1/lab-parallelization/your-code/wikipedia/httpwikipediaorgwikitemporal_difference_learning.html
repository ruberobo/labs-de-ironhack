



Temporal difference learning - Wikipedia





























Temporal difference learning

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search
Part of a series onMachine learninganddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural network
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NeurIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning

vte
Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.[1]
While Monte Carlo methods only adjust their estimates once the final outcome is known, TD methods adjust predictions to match later, more accurate, predictions about the future before the final outcome is known.[2] This is a form of bootstrapping, as illustrated with the following example:

"Suppose you wish to predict the weather for Saturday, and you have some model that predicts Saturday's weather, given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday – and thus be able to change, say, Saturday's model before Saturday arrives."[2]
Temporal difference methods are related to the temporal difference model of animal learning.[3][4][5][6][7]

Contents

1 Mathematical formulation
2 TD-Lambda
3 TD algorithm in neuroscience
4 See also
5 Notes
6 Bibliography
7 External links


Mathematical formulation[edit]
The tabular TD(0) method is one of the simplest TD methods. It is a special case of more general stochastic approximation methods. It estimates the state value function of a finite-state Markov decision process (MDP) under a policy 



π


{\displaystyle \pi }

. Let 




V

π




{\displaystyle V^{\pi }}

 denote the state value function of the MDP with states 



(

s

t



)

t
∈

N





{\displaystyle (s_{t})_{t\in \mathbb {N} }}

, rewards 



(

r

t



)

t
∈

N





{\displaystyle (r_{t})_{t\in \mathbb {N} }}

 and discount rate[8] 



γ


{\displaystyle \gamma }

 under the policy 



π


{\displaystyle \pi }

:






V

π


(
s
)
=

E

a
∼
π



{


∑

t
=
0


∞



γ

t



r

t


(

a

t


)


|



s

0


=
s

}

.


{\displaystyle V^{\pi }(s)=E_{a\sim \pi }\left\{\sum _{t=0}^{\infty }\gamma ^{t}r_{t}(a_{t}){\Bigg |}s_{0}=s\right\}.}


We drop the action from the notion for convenience. 




V

π




{\displaystyle V^{\pi }}

 satisfies the Hamilton-Jacobi-Bellman Equation: 






V

π


(
s
)
=

E

π


{

r

0


+
γ

V

π


(

s

1


)

|


s

0


=
s
}
,


{\displaystyle V^{\pi }(s)=E_{\pi }\{r_{0}+\gamma V^{\pi }(s_{1})|s_{0}=s\},}


so 




r

0


+
γ

V

π


(

s

1


)


{\displaystyle r_{0}+\gamma V^{\pi }(s_{1})}

 is an unbiased estimate for 




V

π


(
s
)


{\displaystyle V^{\pi }(s)}

. This observation motivates the following algorithm for estimating 




V

π




{\displaystyle V^{\pi }}

.
The algorithm starts by initializing a table 



V
(
s
)


{\displaystyle V(s)}

 arbitrarily, with one value for each state of the MDP. A positive learning rate 



α


{\displaystyle \alpha }

 is chosen.
We then repeatedly evaluate the policy 



π


{\displaystyle \pi }

, obtain a reward 



r


{\displaystyle r}

 and update the value function for the old state using the rule:[9]





V
(
s
)
←
V
(
s
)
+
α
(




r
+
γ
V
(

s
′

)

⏞



The TD target


−
V
(
s
)
)


{\displaystyle V(s)\leftarrow V(s)+\alpha (\overbrace {r+\gamma V(s')} ^{\text{The TD target}}-V(s))}


where 



s


{\displaystyle s}

 and 




s
′



{\displaystyle s'}

are the old and new states, respectively. The value 



r
+
γ
V
(

s
′

)


{\displaystyle r+\gamma V(s')}

 is known as the TD target.

TD-Lambda[edit]
TD-Lambda is a learning algorithm invented by Richard S. Sutton based on earlier work on temporal difference learning by Arthur Samuel.[1] This algorithm was famously applied by Gerald Tesauro to create TD-Gammon, a program that learned to play the game of backgammon at the level of expert human players.[10]
The lambda (



λ


{\displaystyle \lambda }

) parameter refers to the trace decay parameter, with 



0
⩽
λ
⩽
1


{\displaystyle 0\leqslant \lambda \leqslant 1}

. Higher settings lead to longer lasting traces; that is, a larger proportion of credit from a reward can be given to more distant states and actions when 



λ


{\displaystyle \lambda }

 is higher, with 



λ
=
1


{\displaystyle \lambda =1}

 producing parallel learning to Monte Carlo RL algorithms.

TD algorithm in neuroscience[edit]
The TD algorithm has also received attention in the field of neuroscience. Researchers discovered that the firing rate of dopamine neurons in the ventral tegmental area (VTA) and substantia nigra (SNc) appear to mimic the error function in the algorithm.[3][4][5][6][7] The error function reports back the difference between the estimated reward at any given state or time step and the actual reward received. The larger the error function, the larger the difference between the expected and actual reward. When this is paired with a stimulus that accurately reflects a future reward, the error can be used to associate the stimulus with the future reward.
Dopamine cells appear to behave in a similar manner. In one experiment measurements of dopamine cells were made while training a monkey to associate a stimulus with the reward of juice.[11] Initially the dopamine cells increased firing rates when the monkey received juice, indicating a difference in expected and actual rewards. Over time this increase in firing back propagated to the earliest reliable stimulus for the reward. Once the monkey was fully trained, there was no increase in firing rate upon presentation of the predicted reward. Continually, the firing rate for the dopamine cells decreased below normal activation when the expected reward was not produced. This mimics closely how the error function in TD is used for reinforcement learning.
The relationship between the model and potential neurological function has produced research attempting to use TD to explain many aspects of behavioral research.[12] It has also been used to study conditions such as schizophrenia or the consequences of pharmacological manipulations of dopamine on learning.[13]

See also[edit]
Q-learning
SARSA
Rescorla-Wagner model
PVLV
Notes[edit]

^ a b Richard Sutton & Andrew Barto (1998). Reinforcement Learning. MIT Press. ISBN 978-0-585-02445-5. Archived from the original on 2017-03-30.

^ a b Richard Sutton (1988). "Learning to predict by the methods of temporal differences". Machine Learning. 3 (1): 9–44. doi:10.1007/BF00115009. (A revised version is available on Richard Sutton's publication page Archived 2017-03-30 at the Wayback Machine)

^ a b Schultz, W, Dayan, P & Montague, PR. (1997). "A neural substrate of prediction and reward". Science. 275 (5306): 1593–1599. CiteSeerX 10.1.1.133.6176. doi:10.1126/science.275.5306.1593. PMID 9054347.CS1 maint: multiple names: authors list (link)

^ a b Montague, P. R.; Dayan, P.; Sejnowski, T. J. (1996-03-01). "A framework for mesencephalic dopamine systems based on predictive Hebbian learning" (PDF). The Journal of Neuroscience. 16 (5): 1936–1947. doi:10.1523/JNEUROSCI.16-05-01936.1996. ISSN 0270-6474. PMC 6578666. PMID 8774460.

^ a b Montague, P.R.; Dayan, P.; Nowlan, S.J.; Pouget, A.; Sejnowski, T.J. (1993). "Using aperiodic reinforcement for directed self-organization" (PDF). Advances in Neural Information Processing Systems. 5: 969–976.

^ a b Montague, P. R.; Sejnowski, T. J. (1994). "The predictive brain: temporal coincidence and temporal order in synaptic learning mechanisms". Learning & Memory. 1 (1): 1–33. ISSN 1072-0502. PMID 10467583.

^ a b Sejnowski, T.J.; Dayan, P.; Montague, P.R. (1995). "Predictive hebbian learning" (PDF). Proceedings of Eighth ACM Conference on Computational Learning Theory: 15–18. doi:10.1145/225298.225300.

^ Discount rate parameter allows for a time preference toward more immediate rewards, and away from distant future rewards

^ Reinforcement learning: An introduction (PDF). p. 130. Archived from the original (PDF) on 2017-07-12.

^ Tesauro, Gerald (March 1995). "Temporal Difference Learning and TD-Gammon". Communications of the ACM. 38 (3): 58–68. doi:10.1145/203330.203343. Retrieved 2010-02-08.

^ Schultz, W. (1998). "Predictive reward signal of dopamine neurons". Journal of Neurophysiology. 80 (1): 1–27. CiteSeerX 10.1.1.408.5994. doi:10.1152/jn.1998.80.1.1. PMID 9658025.

^ Dayan, P. (2001). "Motivated reinforcement learning" (PDF). Advances in Neural Information Processing Systems. MIT Press. 14: 11–18.

^ Smith, A., Li, M., Becker, S. and Kapur, S. (2006). "Dopamine, prediction error, and associative learning: a model-based account". Network: Computation in Neural Systems. 17 (1): 61–84. doi:10.1080/09548980500361624. PMID 16613795.CS1 maint: multiple names: authors list (link)


Bibliography[edit]
Sutton, R.S., Barto A.G. (1990). "Time Derivative Models of Pavlovian Reinforcement" (PDF). Learning and Computational Neuroscience: Foundations of Adaptive Networks: 497–537.CS1 maint: multiple names: authors list (link)
Gerald Tesauro (March 1995). "Temporal Difference Learning and TD-Gammon". Communications of the ACM. 38 (3): 58–68. doi:10.1145/203330.203343.
Imran Ghory. Reinforcement Learning in Board Games.
S. P. Meyn, 2007.  Control Techniques for Complex Networks, Cambridge University Press, 2007. See final chapter, and appendix with abridged Meyn & Tweedie.
External links[edit]
Scholarpedia Temporal difference Learning
TD-Gammon
TD-Networks Research Group
Connect Four TDGravity Applet (+ mobile phone version) – self-learned using TD-Leaf method (combination of TD-Lambda with shallow tree search)
Self Learning Meta-Tic-Tac-Toe Example web app showing how temporal difference learning can be used to learn state evaluation constants for a minimax AI playing a simple board game.
Reinforcement Learning Problem, document explaining how temporal difference learning can be used to speed up Q-learning
TD-Simulator Temporal difference simulator for classical conditioning




Retrieved from "https://en.wikipedia.org/w/index.php?title=Temporal_difference_learning&oldid=972949594"
Categories: Computational neuroscienceReinforcement learningSubtractionHidden categories: Webarchive template wayback linksCS1 maint: multiple names: authors list






Navigation menu




Personal tools




Not logged inTalkContributionsCreate accountLog in






Namespaces




ArticleTalk






Variants












Views




ReadEditView history






More









Search



















Navigation




Main pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonateWikipedia store





Contribute




HelpCommunity portalRecent changesUpload file





Tools




What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageWikidata item





Print/export




Download as PDFPrintable version





Languages




العربيةDeutschفارسیFrançais한국어Italiano
Edit links






 This page was last edited on 14 August 2020, at 16:34 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Mobile view
Developers
Statistics
Cookie statement










