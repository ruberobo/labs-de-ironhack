



Vapnik–Chervonenkis theory - Wikipedia





























Vapnik–Chervonenkis theory

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search
Branch of statistical computational learning theoryPart of a series onMachine learninganddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural network
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NeurIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning

vte
Vapnik–Chervonenkis theory (also known as VC theory) was developed during 1960–1990 by Vladimir Vapnik and Alexey Chervonenkis. The theory is a form of computational learning theory, which attempts to explain the learning process from a statistical point of view.
VC theory is related to statistical learning theory  and to empirical processes.  Richard M. Dudley and Vladimir Vapnik, among others, have applied VC-theory to empirical processes.

Contents

1 Introduction
2 Overview of VC theory in Empirical Processes

2.1 Background on Empirical Processes
2.2 Symmetrization
2.3 VC Connection


3 VC Inequality

3.1 Theorem (VC Inequality)


4 References


Introduction[edit]
VC theory covers at least four parts (as explained in The Nature of Statistical Learning Theory[1]):

Theory of consistency of learning processes
What are (necessary and sufficient) conditions for consistency of a learning process based on the empirical risk minimization principle?
Nonasymptotic theory of the rate of convergence of learning processes
How fast is the rate of convergence of the learning process?
Theory of controlling the generalization ability of learning processes
How can one control the rate of convergence (the generalization ability) of the learning process?
Theory of constructing learning machines
How can one construct algorithms that can control the generalization ability?
VC Theory is a major subbranch of statistical learning theory. One of its main applications in statistical learning theory is to provide generalization conditions for learning algorithms.  From this point of view, VC theory is related to stability, which is an alternative approach for characterizing generalization.
In addition, VC theory and VC dimension are instrumental in the theory of empirical processes, in the case of processes indexed by VC classes. Arguably these are the most important applications of the VC theory, and are employed in proving generalization. Several techniques will be introduced that are widely used in the empirical process and VC theory. The discussion is mainly based on the book Weak Convergence and Empirical Processes: With Applications to Statistics.[2]

Overview of VC theory in Empirical Processes[edit]
Background on Empirical Processes[edit]
Let 




X

1


,
…
,

X

n




{\displaystyle X_{1},\ldots ,X_{n}}

 be random elements defined on a measurable space 



(


X


,


A


)


{\displaystyle ({\mathcal {X}},{\mathcal {A}})}

. For any measure 



Q


{\displaystyle Q}

 on 



(


X


,


A


)


{\displaystyle ({\mathcal {X}},{\mathcal {A}})}

, and any measurable functions 



f
:


X


→

R



{\displaystyle f:{\mathcal {X}}\to \mathbf {R} }

, define





Q
f
=
∫
f
d
Q


{\displaystyle Qf=\int fdQ}


Measurability issues will be ignored here, for more technical detail see [3]. Let 





F




{\displaystyle {\mathcal {F}}}

 be a class of measurable functions 



f
:


X


→

R



{\displaystyle f:{\mathcal {X}}\to \mathbf {R} }

 and define:





‖
Q

‖


F



=
sup
{
|
Q
f
|
 
:
 
f
∈


F


}
.


{\displaystyle \|Q\|_{\mathcal {F}}=\sup\{\vert Qf\vert \ :\ f\in {\mathcal {F}}\}.}


Define the empirical measure







P


n


=

n

−
1



∑

i
=
1


n



δ


X

i




,


{\displaystyle \mathbb {P} _{n}=n^{-1}\sum _{i=1}^{n}\delta _{X_{i}},}


where δ here stands for the Dirac measure. The empirical measure induces a map 





F


→

R



{\displaystyle {\mathcal {F}}\to \mathbf {R} }

 given by:





f
↦


P


n


f
=


1
n


(
f
(

X

1


)
+
.
.
.
+
f
(

X

n


)
)


{\displaystyle f\mapsto \mathbb {P} _{n}f={\frac {1}{n}}(f(X_{1})+...+f(X_{n}))}


Now suppose P is the underlying true distribution of the data, which is unknown. Empirical Processes theory aims at identifying classes 





F




{\displaystyle {\mathcal {F}}}

 for which statements such as the following hold:

uniform law of large numbers:



‖


P


n


−
P

‖


F





→
n


0
,


{\displaystyle \|\mathbb {P} _{n}-P\|_{\mathcal {F}}{\underset {n}{\to }}0,}


That is, as 



n
→
∞


{\displaystyle n\to \infty }

,





|



1
n


(
f
(

X

1


)
+
.
.
.
+
f
(

X

n


)
)
−
∫
f
d
P

|

→
0


{\displaystyle \left|{\frac {1}{n}}(f(X_{1})+...+f(X_{n}))-\int fdP\right|\to 0}


uniformly for all 



f
∈


F




{\displaystyle f\in {\mathcal {F}}}

.
uniform central limit theorem:






G


n


=


n


(


P


n


−
P
)
⇝

G

,


in 


ℓ

∞


(


F


)


{\displaystyle \mathbb {G} _{n}={\sqrt {n}}(\mathbb {P} _{n}-P)\rightsquigarrow \mathbb {G} ,\quad {\text{in }}\ell ^{\infty }({\mathcal {F}})}


In the former case 





F




{\displaystyle {\mathcal {F}}}

 is called Glivenko-Cantelli class, and in the latter case (under the assumption 



∀
x
,

sup

f
∈


F




|
f
(
x
)
−
P
f
|
<
∞


{\displaystyle \forall x,\sup \nolimits _{f\in {\mathcal {F}}}\vert f(x)-Pf\vert <\infty }

) the class 





F




{\displaystyle {\mathcal {F}}}

 is called Donsker or P-Donsker. A Donsker class is Glivenko-Cantelli in probability by an application of Slutsky's theorem .
These statements are true for a single 



f


{\displaystyle f}

, by standard LLN, CLT arguments under regularity conditions, and the difficulty in the Empirical Processes comes in because joint statements are being made for all 



f
∈


F




{\displaystyle f\in {\mathcal {F}}}

. Intuitively then, the set 





F




{\displaystyle {\mathcal {F}}}

 cannot be too large, and as it turns out that the geometry of 





F




{\displaystyle {\mathcal {F}}}

 plays a very important role.
One way of measuring how big the function set 





F




{\displaystyle {\mathcal {F}}}

 is to use the so-called covering numbers. The covering number





N
(
ε
,


F


,
‖
⋅
‖
)


{\displaystyle N(\varepsilon ,{\mathcal {F}},\|\cdot \|)}


is the minimal number of balls 



{
g
:
‖
g
−
f
‖
<
ε
}


{\displaystyle \{g:\|g-f\|<\varepsilon \}}

 needed to cover the set 





F




{\displaystyle {\mathcal {F}}}

 (here it is obviously assumed that there is an underlying norm on 





F




{\displaystyle {\mathcal {F}}}

).  The entropy is the logarithm of the covering number.
Two sufficient conditions are provided below, under which it can be proved that the set 





F




{\displaystyle {\mathcal {F}}}

 is Glivenko-Cantelli or Donsker.
A class 





F




{\displaystyle {\mathcal {F}}}

 is P-Glivenko-Cantelli if it is P-measurable with envelope F such that 




P

∗


F
<
∞


{\displaystyle P^{\ast }F<\infty }

 and satisfies:





∀
ε
>
0


sup

Q


N
(
ε
‖
F

‖

Q


,


F


,

L

1


(
Q
)
)
<
∞
.


{\displaystyle \forall \varepsilon >0\quad \sup \nolimits _{Q}N(\varepsilon \|F\|_{Q},{\mathcal {F}},L_{1}(Q))<\infty .}


The next condition is a version of the celebrated Dudley's theorem. If 





F




{\displaystyle {\mathcal {F}}}

 is a class of functions such that






∫

0


∞



sup

Q




log
⁡
N

(

ε
‖
F

‖

Q
,
2


,


F


,

L

2


(
Q
)

)



d
ε
<
∞


{\displaystyle \int _{0}^{\infty }\sup \nolimits _{Q}{\sqrt {\log N\left(\varepsilon \|F\|_{Q,2},{\mathcal {F}},L_{2}(Q)\right)}}d\varepsilon <\infty }


then 





F




{\displaystyle {\mathcal {F}}}

 is P-Donsker for every probability measure P such that 




P

∗



F

2


<
∞


{\displaystyle P^{\ast }F^{2}<\infty }

. In the last integral, the notation means





‖
f

‖

Q
,
2


=


(

∫

|

f


|


2


d
Q

)



1
2





{\displaystyle \|f\|_{Q,2}=\left(\int |f|^{2}dQ\right)^{\frac {1}{2}}}

.
Symmetrization[edit]
The majority of the arguments of how to bound the empirical process, rely on symmetrization, maximal and concentration inequalities and chaining.  Symmetrization is usually the first step of the proofs, and since it is used in many machine learning proofs on bounding empirical loss functions (including the proof of the VC inequality which is discussed in the next section) it is presented here.
Consider the empirical process:





f
↦
(


P


n


−
P
)
f
=



1
n




∑

i
=
1


n


(
f
(

X

i


)
−
P
f
)


{\displaystyle f\mapsto (\mathbb {P} _{n}-P)f={\dfrac {1}{n}}\sum _{i=1}^{n}(f(X_{i})-Pf)}


Turns out that there is a connection between the empirical and the following symmetrized process:





f
↦


P


n


0


f
=



1
n




∑

i
=
1


n



ε

i


f
(

X

i


)


{\displaystyle f\mapsto \mathbb {P} _{n}^{0}f={\dfrac {1}{n}}\sum _{i=1}^{n}\varepsilon _{i}f(X_{i})}


The symmetrized process is a Rademacher process, conditionally on the data 




X

i




{\displaystyle X_{i}}

. Therefore, it is a sub-Gaussian process by Hoeffding's inequality.
Lemma (Symmetrization). For every nondecreasing, convex Φ: R → R and class of measurable functions 





F




{\displaystyle {\mathcal {F}}}

,






E

Φ
(
‖


P


n


−
P

‖


F



)
≤

E

Φ

(

2


‖


P


n


0


‖



F




)



{\displaystyle \mathbb {E} \Phi (\|\mathbb {P} _{n}-P\|_{\mathcal {F}})\leq \mathbb {E} \Phi \left(2\left\|\mathbb {P} _{n}^{0}\right\|_{\mathcal {F}}\right)}


The proof of the Symmetrization lemma relies on introducing independent copies of the original variables 




X

i




{\displaystyle X_{i}}

 (sometimes referred to as a ghost sample) and replacing the inner expectation of the LHS by these copies. After an application of Jensen's inequality different signs could be introduced (hence the name symmetrization) without changing the expectation. The proof can be found below because of its instructive nature.


[Proof]

Introduce the "ghost sample" 




Y

1


,
…
,

Y

n




{\displaystyle Y_{1},\ldots ,Y_{n}}

 to be independent copies of 




X

1


,
…
,

X

n




{\displaystyle X_{1},\ldots ,X_{n}}

. For fixed values of 




X

1


,
…
,

X

n




{\displaystyle X_{1},\ldots ,X_{n}}

 one has:





‖


P


n


−
P

‖


F



=

sup

f
∈


F







1
n




|


∑

i
=
1


n


f
(

X

i


)
−

E

f
(

Y

i


)

|

≤


E


Y



sup

f
∈


F







1
n




|


∑

i
=
1


n


f
(

X

i


)
−
f
(

Y

i


)

|



{\displaystyle \|\mathbb {P} _{n}-P\|_{\mathcal {F}}=\sup _{f\in {\mathcal {F}}}{\dfrac {1}{n}}\left|\sum _{i=1}^{n}f(X_{i})-\mathbb {E} f(Y_{i})\right|\leq \mathbb {E} _{Y}\sup _{f\in {\mathcal {F}}}{\dfrac {1}{n}}\left|\sum _{i=1}^{n}f(X_{i})-f(Y_{i})\right|}


Therefore, by Jensen's inequality:





Φ
(
‖


P


n


−
P

‖


F



)
≤


E


Y


Φ

(


‖




1
n




∑

i
=
1


n


f
(

X

i


)
−
f
(

Y

i


)

‖



F



)



{\displaystyle \Phi (\|\mathbb {P} _{n}-P\|_{\mathcal {F}})\leq \mathbb {E} _{Y}\Phi \left(\left\|{\dfrac {1}{n}}\sum _{i=1}^{n}f(X_{i})-f(Y_{i})\right\|_{\mathcal {F}}\right)}


Taking expectation with respect to 



X


{\displaystyle X}

 gives:






E

Φ
(
‖


P


n


−
P

‖


F



)
≤


E


X




E


Y


Φ

(


‖




1
n




∑

i
=
1


n


f
(

X

i


)
−
f
(

Y

i


)

‖



F



)



{\displaystyle \mathbb {E} \Phi (\|\mathbb {P} _{n}-P\|_{\mathcal {F}})\leq \mathbb {E} _{X}\mathbb {E} _{Y}\Phi \left(\left\|{\dfrac {1}{n}}\sum _{i=1}^{n}f(X_{i})-f(Y_{i})\right\|_{\mathcal {F}}\right)}


Note that adding a minus sign in front of a term 



f
(

X

i


)
−
f
(

Y

i


)


{\displaystyle f(X_{i})-f(Y_{i})}

 doesn't change the RHS, because it's a symmetric function of 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

. Therefore, the RHS remains the same under "sign perturbation":






E

Φ

(


‖




1
n




∑

i
=
1


n



e

i



(

f
(

X

i


)
−
f
(

Y

i


)

)


‖



F



)



{\displaystyle \mathbb {E} \Phi \left(\left\|{\dfrac {1}{n}}\sum _{i=1}^{n}e_{i}\left(f(X_{i})-f(Y_{i})\right)\right\|_{\mathcal {F}}\right)}


for any 



(

e

1


,

e

2


,
…
,

e

n


)
∈
{
−
1
,
1

}

n




{\displaystyle (e_{1},e_{2},\ldots ,e_{n})\in \{-1,1\}^{n}}

. Therefore:






E

Φ
(
‖


P


n


−
P

‖


F



)
≤


E


ε



E

Φ

(


‖




1
n




∑

i
=
1


n



ε

i



(

f
(

X

i


)
−
f
(

Y

i


)

)


‖



F



)



{\displaystyle \mathbb {E} \Phi (\|\mathbb {P} _{n}-P\|_{\mathcal {F}})\leq \mathbb {E} _{\varepsilon }\mathbb {E} \Phi \left(\left\|{\dfrac {1}{n}}\sum _{i=1}^{n}\varepsilon _{i}\left(f(X_{i})-f(Y_{i})\right)\right\|_{\mathcal {F}}\right)}


Finally using first triangle inequality and then convexity of 



Φ


{\displaystyle \Phi }

 gives:






E

Φ
(
‖


P


n


−
P

‖


F



)
≤



1
2





E


ε



E

Φ

(

2


‖




1
n




∑

i
=
1


n



ε

i


f
(

X

i


)

‖



F




)

+



1
2





E


ε



E

Φ

(

2


‖




1
n




∑

i
=
1


n



ε

i


f
(

Y

i


)

‖



F




)



{\displaystyle \mathbb {E} \Phi (\|\mathbb {P} _{n}-P\|_{\mathcal {F}})\leq {\dfrac {1}{2}}\mathbb {E} _{\varepsilon }\mathbb {E} \Phi \left(2\left\|{\dfrac {1}{n}}\sum _{i=1}^{n}\varepsilon _{i}f(X_{i})\right\|_{\mathcal {F}}\right)+{\dfrac {1}{2}}\mathbb {E} _{\varepsilon }\mathbb {E} \Phi \left(2\left\|{\dfrac {1}{n}}\sum _{i=1}^{n}\varepsilon _{i}f(Y_{i})\right\|_{\mathcal {F}}\right)}


Where the last two expressions on the RHS are the same, which concludes the proof.



A typical way of proving empirical CLTs, first uses symmetrization to pass the empirical process to 





P


n


0




{\displaystyle \mathbb {P} _{n}^{0}}

 and then argue conditionally on the data, using the fact that Rademacher processes are simple processes with nice properties.

VC Connection[edit]
It turns out that there is a fascinating connection between certain combinatorial properties of the set 





F




{\displaystyle {\mathcal {F}}}

 and the entropy numbers. Uniform  covering numbers can be controlled by the notion of Vapnik-Chervonenkis classes of sets - or shortly VC sets.
Consider a collection  





C




{\displaystyle {\mathcal {C}}}

 of subsets of the sample space 





X




{\displaystyle {\mathcal {X}}}

. 





C




{\displaystyle {\mathcal {C}}}

 is said to pick out a certain subset 



W


{\displaystyle W}

 of the finite set 



S
=
{

x

1


,
…
,

x

n


}
⊂


X




{\displaystyle S=\{x_{1},\ldots ,x_{n}\}\subset {\mathcal {X}}}

 if 



W
=
S
∩
C


{\displaystyle W=S\cap C}

 for some 



C
∈


C




{\displaystyle C\in {\mathcal {C}}}

. 





C




{\displaystyle {\mathcal {C}}}

 is said to shatter S if it picks out each of its 2n subsets. The VC-index (similar to VC dimension + 1 for an appropriately chosen classifier set) 



V
(


C


)


{\displaystyle V({\mathcal {C}})}

 of 





C




{\displaystyle {\mathcal {C}}}

 is the smallest n for which no set of size n is shattered by 





C




{\displaystyle {\mathcal {C}}}

.
Sauer's lemma then states that the number 




Δ

n


(


C


,

x

1


,
…
,

x

n


)


{\displaystyle \Delta _{n}({\mathcal {C}},x_{1},\ldots ,x_{n})}

 of subsets picked out by a VC-class 





C




{\displaystyle {\mathcal {C}}}

 satisfies:






max


x

1


,
…
,

x

n





Δ

n


(


C


,

x

1


,
…
,

x

n


)
≤

∑

j
=
0


V
(


C


)
−
1





(


n
j


)



≤


(



n
e


V
(


C


)
−
1



)


V
(


C


)
−
1




{\displaystyle \max _{x_{1},\ldots ,x_{n}}\Delta _{n}({\mathcal {C}},x_{1},\ldots ,x_{n})\leq \sum _{j=0}^{V({\mathcal {C}})-1}{n \choose j}\leq \left({\frac {ne}{V({\mathcal {C}})-1}}\right)^{V({\mathcal {C}})-1}}


Which is a polynomial number 



O
(

n

V
(


C


)
−
1


)


{\displaystyle O(n^{V({\mathcal {C}})-1})}

 of subsets rather than an exponential number. Intuitively this means that a finite VC-index implies that 





C




{\displaystyle {\mathcal {C}}}

 has an apparent simplistic structure.
A similar bound can be shown (with a different constant, same rate) for the so-called VC subgraph classes. For a function 



f
:


X


→

R



{\displaystyle f:{\mathcal {X}}\to \mathbf {R} }

 the subgraph is a subset of 





X


×

R



{\displaystyle {\mathcal {X}}\times \mathbf {R} }

 such that: 



{
(
x
,
t
)
:
t
<
f
(
x
)
}


{\displaystyle \{(x,t):t<f(x)\}}

. A collection of 





F




{\displaystyle {\mathcal {F}}}

 is called a VC subgraph class if all subgraphs form a VC-class.
Consider a set of indicator functions 






I




C



=
{

1

C


:
C
∈


C


}


{\displaystyle {\mathcal {I}}_{\mathcal {C}}=\{1_{C}:C\in {\mathcal {C}}\}}

 in 




L

1


(
Q
)


{\displaystyle L_{1}(Q)}

 for discrete empirical type of measure Q (or equivalently for any probability measure Q). It can then be shown that quite remarkably, for 



r
≥
1


{\displaystyle r\geq 1}

:





N
(
ε
,



I




C



,

L

r


(
Q
)
)
≤
K
V
(


C


)
(
4
e

)

V
(


C


)



ε

−
r
(
V
(


C


)
−
1
)




{\displaystyle N(\varepsilon ,{\mathcal {I}}_{\mathcal {C}},L_{r}(Q))\leq KV({\mathcal {C}})(4e)^{V({\mathcal {C}})}\varepsilon ^{-r(V({\mathcal {C}})-1)}}


Further consider the symmetric convex hull of a set 





F




{\displaystyle {\mathcal {F}}}

: 



sconv
⁡


F




{\displaystyle \operatorname {sconv} {\mathcal {F}}}

 being the collection of functions of the form 




∑

i
=
1


m



α

i



f

i




{\displaystyle \sum _{i=1}^{m}\alpha _{i}f_{i}}

 with 




∑

i
=
1


m



|


α

i



|

≤
1


{\displaystyle \sum _{i=1}^{m}|\alpha _{i}|\leq 1}

. Then if





N

(

ε
‖
F

‖

Q
,
2


,


F


,

L

2


(
Q
)

)

≤
C

ε

−
V




{\displaystyle N\left(\varepsilon \|F\|_{Q,2},{\mathcal {F}},L_{2}(Q)\right)\leq C\varepsilon ^{-V}}


the following is valid for the convex hull of 





F




{\displaystyle {\mathcal {F}}}

:





log
⁡
N

(

ε
‖
F

‖

Q
,
2


,
sconv
⁡


F


,

L

2


(
Q
)

)

≤
K

ε

−



2
V


V
+
2







{\displaystyle \log N\left(\varepsilon \|F\|_{Q,2},\operatorname {sconv} {\mathcal {F}},L_{2}(Q)\right)\leq K\varepsilon ^{-{\frac {2V}{V+2}}}}


The important consequence of this fact is that








2
V


V
+
2



>
2
,


{\displaystyle {\frac {2V}{V+2}}>2,}


which is just enough so that the entropy integral is going to converge, and therefore the class 



sconv
⁡


F




{\displaystyle \operatorname {sconv} {\mathcal {F}}}

 is going to be P-Donsker.
Finally an example of a VC-subgraph class is considered. Any finite-dimensional vector space 





F




{\displaystyle {\mathcal {F}}}

 of measurable functions 



f
:


X


→

R



{\displaystyle f:{\mathcal {X}}\to \mathbf {R} }

 is VC-subgraph of index smaller than or equal to 



dim
⁡
(


F


)
+
2


{\displaystyle \dim({\mathcal {F}})+2}

.


[Proof]

Take 



n
=
dim
⁡
(


F


)
+
2


{\displaystyle n=\dim({\mathcal {F}})+2}

 points 



(

x

1


,

t

1


)
,
…
,
(

x

n


,

t

n


)


{\displaystyle (x_{1},t_{1}),\ldots ,(x_{n},t_{n})}

. The vectors:





(
f
(

x

1


)
,
…
,
f
(

x

n


)
)
−
(

t

1


,
…
,

t

n


)


{\displaystyle (f(x_{1}),\ldots ,f(x_{n}))-(t_{1},\ldots ,t_{n})}


are in a n − 1 dimensional subspace of Rn. Take a ≠ 0, a vector that is orthogonal to this subspace. Therefore:






∑


a

i


>
0



a

i


(
f
(

x

i


)
−

t

i


)
=

∑


a

i


<
0


(
−

a

i


)
(
f
(

x

i


)
−

t

i


)
,

∀
f
∈


F




{\displaystyle \sum _{a_{i}>0}a_{i}(f(x_{i})-t_{i})=\sum _{a_{i}<0}(-a_{i})(f(x_{i})-t_{i}),\quad \forall f\in {\mathcal {F}}}


Consider the set 



S
=
{
(

x

i


,

t

i


)
:

a

i


>
0
}


{\displaystyle S=\{(x_{i},t_{i}):a_{i}>0\}}

. This set cannot be picked out since if there is some 



f


{\displaystyle f}

 such that 



S
=
{
(

x

i


,

t

i


)
:
f
(

x

i


)
>

t

i


}


{\displaystyle S=\{(x_{i},t_{i}):f(x_{i})>t_{i}\}}

 that would imply that the LHS is strictly positive but the RHS is non-positive.



There are generalizations of the notion VC subgraph class, e.g. there is the notion of pseudo-dimension. The interested reader can look into[4].

VC Inequality[edit]
A similar setting is considered, which is more common to machine learning. Let 





X




{\displaystyle {\mathcal {X}}}

 is a feature space and 





Y


=
{
0
,
1
}


{\displaystyle {\mathcal {Y}}=\{0,1\}}

. A function 



f
:


X


→


Y




{\displaystyle f:{\mathcal {X}}\to {\mathcal {Y}}}

 is called a classifier. Let 





F




{\displaystyle {\mathcal {F}}}

 be a set of classifiers. Similarly to the previous section, define the shattering coefficient (also known as growth function):





S
(


F


,
n
)
=

max


x

1


,
…
,

x

n





|

{
(
f
(

x

1


)
,
…
,
f
(

x

n


)
)
,
f
∈


F


}

|



{\displaystyle S({\mathcal {F}},n)=\max _{x_{1},\ldots ,x_{n}}|\{(f(x_{1}),\ldots ,f(x_{n})),f\in {\mathcal {F}}\}|}


Note here that there is a 1:1 go  between each of the functions in 





F




{\displaystyle {\mathcal {F}}}

 and the set on which the function is 1. We can thus define 





C




{\displaystyle {\mathcal {C}}}

 to be the collection of subsets obtained from the above mapping for every 



f
∈


F




{\displaystyle f\in {\mathcal {F}}}

. Therefore, in terms of the previous section the shattering coefficient is precisely






max


x

1


,
…
,

x

n





Δ

n


(


C


,

x

1


,
…
,

x

n


)


{\displaystyle \max _{x_{1},\ldots ,x_{n}}\Delta _{n}({\mathcal {C}},x_{1},\ldots ,x_{n})}

.
This equivalence together with Sauer's Lemma implies that 



S
(


F


,
n
)


{\displaystyle S({\mathcal {F}},n)}

 is going to be polynomial in n, for sufficiently large n provided that the collection 





C




{\displaystyle {\mathcal {C}}}

 has a finite VC-index.
Let 




D

n


=
{
(

X

1


,

Y

1


)
,
…
,
(

X

n


,

Y

m


)
}


{\displaystyle D_{n}=\{(X_{1},Y_{1}),\ldots ,(X_{n},Y_{m})\}}

 is an observed dataset. Assume that the data is generated by an unknown probability distribution 




P

X
Y




{\displaystyle P_{XY}}

. Define 



R
(
f
)
=
P
(
f
(
X
)
≠
Y
)


{\displaystyle R(f)=P(f(X)\neq Y)}

 to be the expected 0/1 loss. Of course since 




P

X
Y




{\displaystyle P_{XY}}

 is unknown in general, one has no access to 



R
(
f
)


{\displaystyle R(f)}

. However the empirical risk, given by:









R
^




n


(
f
)
=



1
n




∑

i
=
1


n



I

(
f
(

X

i


)
≠

Y

i


)


{\displaystyle {\hat {R}}_{n}(f)={\dfrac {1}{n}}\sum _{i=1}^{n}\mathbb {I} (f(X_{i})\neq Y_{i})}


can certainly be evaluated. Then one has the following Theorem:

Theorem (VC Inequality)[edit]
For binary classification and the 0/1 loss function we have the following generalization bounds:









P

(


sup

f
∈


F





|





R
^




n


(
f
)
−
R
(
f
)

|

>
ε

)




≤
8
S
(


F


,
n
)

e

−
n

ε

2



/

32







E


[


sup

f
∈


F





|





R
^




n


(
f
)
−
R
(
f
)

|


]




≤
2





log
⁡
S
(


F


,
n
)
+
log
⁡
2

n










{\displaystyle {\begin{aligned}P\left(\sup _{f\in {\mathcal {F}}}\left|{\hat {R}}_{n}(f)-R(f)\right|>\varepsilon \right)&\leq 8S({\mathcal {F}},n)e^{-n\varepsilon ^{2}/32}\\\mathbb {E} \left[\sup _{f\in {\mathcal {F}}}\left|{\hat {R}}_{n}(f)-R(f)\right|\right]&\leq 2{\sqrt {\dfrac {\log S({\mathcal {F}},n)+\log 2}{n}}}\end{aligned}}}


In words the VC inequality is saying that as the sample increases, provided that 





F




{\displaystyle {\mathcal {F}}}

 has a finite VC dimension, the empirical 0/1 risk becomes a good proxy for the expected 0/1 risk. Note that both RHS of the two inequalities will converge to 0, provided that 



S
(


F


,
n
)


{\displaystyle S({\mathcal {F}},n)}

 grows polynomially in n.
The connection between this framework and the Empirical Process framework is evident. Here one is dealing with a modified empirical process







|





R
^




n


−
R

|



F





{\displaystyle \left|{\hat {R}}_{n}-R\right|_{\mathcal {F}}}


but not surprisingly the ideas are the same. The proof of the (first part of) VC inequality, relies on symmetrization, and then argue conditionally on the data using concentration inequalities (in particular Hoeffding's inequality). The interested reader can check the book [5] Theorems 12.4 and 12.5.

References[edit]
^ Vapnik, Vladimir N (2000). The Nature of Statistical Learning Theory. Information Science and Statistics. Springer-Verlag. ISBN 978-0-387-98780-4.
Vapnik, Vladimir N (1989). Statistical Learning Theory. Wiley-Interscience. ISBN 978-0-471-03003-4.
^ van der Vaart, Aad W.; Wellner, Jon A. (2000). Weak Convergence and Empirical Processes: With Applications to Statistics (2nd ed.). Springer. ISBN 978-0-387-94640-5.
^ Gyorfi, L.; Devroye, L.; Lugosi, G. (1996). A probabilistic theory of pattern recognition (1st ed.). Springer. ISBN 978-0387946184.
See references in articles: Richard M. Dudley, empirical processes, Shattered set.
^ Pollard, David (1990). Empirical Processes: Theory and Applications. NSF-CBMS Regional Conference Series in Probability and Statistics Volume 2. ISBN 978-0-940600-16-4.
Bousquet, O.; Boucheron, S.; Lugosi, G. (2004). "Introduction to Statistical Learning Theory".  In O. Bousquet; U. von Luxburg; G. Ratsch (eds.). Advanced Lectures on Machine Learning. Lecture Notes in Artificial Intelligence. 3176. Springer. pp. 169–207.
Vapnik, V.; Chervonenkis, A. (2004). "On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities". Theory Probab. Appl. 16 (2): 264–280. doi:10.1137/1116025.




Retrieved from "https://en.wikipedia.org/w/index.php?title=Vapnik–Chervonenkis_theory&oldid=965904360"
Categories: Computational learning theoryEmpirical processHidden categories: Articles with short descriptionShort description is different from Wikidata






Navigation menu




Personal tools




Not logged inTalkContributionsCreate accountLog in






Namespaces




ArticleTalk






Variants












Views




ReadEditView history






More









Search



















Navigation




Main pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonateWikipedia store





Contribute




HelpCommunity portalRecent changesUpload file





Tools




What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageWikidata item





Print/export




Download as PDFPrintable version





Languages




FrançaisMagyarУкраїнська中文
Edit links






 This page was last edited on 4 July 2020, at 03:24 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Mobile view
Developers
Statistics
Cookie statement










