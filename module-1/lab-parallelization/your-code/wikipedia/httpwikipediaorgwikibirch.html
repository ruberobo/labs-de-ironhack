



BIRCH - Wikipedia





























BIRCH

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search
This article is about the clustering algorithm. For the tree, see Birch. For other uses, see Birch (disambiguation).
Part of a series onMachine learninganddata mining
Problems
Classification
Clustering
Regression
Anomaly detection
AutoML
Association rules
Reinforcement learning
Structured prediction
Feature engineering
Feature learning
Online learning
Semi-supervised learning
Unsupervised learning
Learning to rank
Grammar induction


Supervised learning(classification • regression) 
Decision trees
Ensembles
Bagging
Boosting
Random forest
k-NN
Linear regression
Naive Bayes
Artificial neural networks
Logistic regression
Perceptron
Relevance vector machine (RVM)
Support vector machine (SVM)


Clustering
BIRCH
CURE
Hierarchical
k-means
Expectation–maximization (EM)
DBSCAN
OPTICS
Mean-shift


Dimensionality reduction
Factor analysis
CCA
ICA
LDA
NMF
PCA
PGD
t-SNE


Structured prediction
Graphical models
Bayes net
Conditional random field
Hidden Markov


Anomaly detection
k-NN
Local outlier factor


Artificial neural network
Autoencoder
Deep learning
DeepDream
Multilayer perceptron
RNN
LSTM
GRU
Restricted Boltzmann machine
GAN
SOM
Convolutional neural network
U-Net


Reinforcement learning
Q-learning
SARSA
Temporal difference (TD)


Theory
Bias–variance dilemma
Computational learning theory
Empirical risk minimization
Occam learning
PAC learning
Statistical learning
VC theory


Machine-learning venues
NeurIPS
ICML
ML
JMLR
ArXiv:cs.LG


Glossary of artificial intelligence
Glossary of artificial intelligence


Related articles
List of datasets for machine-learning research
Outline of machine learning

vte
BIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets.[1] An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database.
Its inventors claim BIRCH to be the "first clustering algorithm proposed in the database area to handle 'noise' (data points that are not part of the underlying pattern) effectively",[1] beating DBSCAN by two months. The algorithm received the SIGMOD 10 year test of time award in 2006.[2]

Contents

1 Problem with previous methods
2 Advantages with BIRCH
3 Algorithm

3.1 Calculations with the cluster features


4 Notes


Problem with previous methods[edit]
Previous clustering algorithms performed less effectively over very large databases and did not adequately consider the case wherein a data-set was too large to fit in main memory. As a result, there was a lot of overhead maintaining high clustering quality while minimizing the cost of additional IO (input/output) operations. Furthermore, most of BIRCH's predecessors inspect all data points (or all currently existing clusters) equally for each 'clustering decision' and do not perform heuristic weighting based on the distance between these data points.

Advantages with BIRCH[edit]
It is local in that each clustering decision is made without scanning all data points and currently existing clusters.
It exploits the observation that the data space is not usually uniformly occupied and not every data point is equally important.
It makes full use of available memory to derive the finest possible sub-clusters while minimizing I/O costs.
It is also an incremental method that does not require the whole data set in advance.

Algorithm[edit]
The BIRCH algorithm takes as input a set of N data points, represented as real-valued vectors, and a desired number of clusters K. It operates in four phases, the second of which is optional.
The first phase builds a clustering feature (



C
F


{\displaystyle CF}

) tree out of the data points, a height-balanced tree data structure, defined as follows:

Given a set of N d-dimensional data points, the clustering feature 



C
F


{\displaystyle CF}

 of the set is defined as the triple 



C
F
=
(
N
,



L
S

→


,
S
S
)


{\displaystyle CF=(N,{\overrightarrow {LS}},SS)}

, where 






L
S

→


=

∑

i
=
1


N





X

i


→




{\displaystyle {\overrightarrow {LS}}=\sum _{i=1}^{N}{\overrightarrow {X_{i}}}}

 is the linear sum and 



S
S
=

∑

i
=
1


N


(



X

i


→



)

2




{\displaystyle SS=\sum _{i=1}^{N}({\overrightarrow {X_{i}}})^{2}}

 is the square sum of data points.
Clustering features are organized in a CF tree, a height-balanced tree with two parameters:[clarification needed] branching factor 



B


{\displaystyle B}

 and threshold 



T


{\displaystyle T}

. Each non-leaf node contains at most 



B


{\displaystyle B}

 entries of the form 



[
C

F

i


,
c
h
i
l

d

i


]


{\displaystyle [CF_{i},child_{i}]}

, where 



c
h
i
l

d

i




{\displaystyle child_{i}}

 is a pointer to its 



i


{\displaystyle i}

th child node and 



C

F

i




{\displaystyle CF_{i}}

 the clustering feature representing the associated subcluster. A leaf node contains at most 



L


{\displaystyle L}

 entries each of the form 



[
C

F

i


]


{\displaystyle [CF_{i}]}

 . It also has two pointers prev and next which are used to chain all leaf nodes together. The tree size depends on the parameter 



T


{\displaystyle T}

. A node is required to fit in a page of size 



P


{\displaystyle P}

. 



B


{\displaystyle B}

 and 



L


{\displaystyle L}

 are determined by 



P


{\displaystyle P}

. So 



P


{\displaystyle P}

 can be varied for performance tuning. It is a very compact representation of the dataset because each entry in a leaf node is not a single data point but a subcluster.
In the second step, the algorithm scans all the leaf entries in the initial 



C
F


{\displaystyle CF}

 tree to rebuild a smaller 



C
F


{\displaystyle CF}

 tree, while removing outliers and grouping crowded subclusters into larger ones. This step is marked optional in the original presentation of BIRCH.
In step three an existing clustering algorithm is used to cluster all leaf entries. Here an agglomerative hierarchical clustering algorithm is applied directly to the subclusters represented by their 



C
F


{\displaystyle CF}

 vectors. It also provides the flexibility of allowing the user to specify either the desired number of clusters or the desired diameter threshold for clusters. After this step a set of clusters is obtained that captures major distribution pattern in the data. However, there might exist minor and localized inaccuracies which can be handled by an optional step 4. In step 4 the centroids of the clusters produced in step 3 are used as seeds and redistribute the data points to its closest seeds to obtain a new set of clusters. Step 4 also provides us with an option of discarding outliers. That is a point which is too far from its closest seed can be treated as an outlier.

Calculations with the cluster features[edit]
Given only the 



C
F
=
[
N
,



L
S

→


,
S
S
]


{\displaystyle CF=[N,{\overrightarrow {LS}},SS]}

, the same measures can be calculated without the knowledge of the underlying actual values.

Centroid: 





C
→


=




∑

i
=
1


N





X

i


→



N


=




L
S

→

N




{\displaystyle {\overrightarrow {C}}={\frac {\sum _{i=1}^{N}{\overrightarrow {X_{i}}}}{N}}={\frac {\overrightarrow {LS}}{N}}}


Radius: 



R
=





∑

i
=
1


N


(



X

i


→


−


C
→



)

2



N



=




N
⋅



C
→



2


+
S
S
−
2
⋅


C
→


⋅



L
S

→



N



=





S
S

N


−
(




L
S

→

N



)

2






{\displaystyle R={\sqrt {\frac {\sum _{i=1}^{N}({\overrightarrow {X_{i}}}-{\overrightarrow {C}})^{2}}{N}}}={\sqrt {\frac {N\cdot {\overrightarrow {C}}^{2}+SS-2\cdot {\overrightarrow {C}}\cdot {\overrightarrow {LS}}}{N}}}={\sqrt {{\frac {SS}{N}}-({\frac {\overrightarrow {LS}}{N}})^{2}}}}


Average Linkage Distance between clusters 



C

F

1


=
[

N

1


,



L

S

1



→


,
S

S

1


]


{\displaystyle CF_{1}=[N_{1},{\overrightarrow {LS_{1}}},SS_{1}]}

 and 



C

F

2


=
[

N

2


,



L

S

2



→


,
S

S

2


]


{\displaystyle CF_{2}=[N_{2},{\overrightarrow {LS_{2}}},SS_{2}]}

:




D

2


=





∑

i
=
1



N

1





∑

j
=
1



N

2




(



X

i


→


−



Y

j


→



)

2





N

1


⋅

N

2






=





N

1


⋅
S

S

2


+

N

2


⋅
S

S

1


−
2
⋅



L

S

1



→


⋅



L

S

2



→





N

1


⋅

N

2








{\displaystyle D_{2}={\sqrt {\frac {\sum _{i=1}^{N_{1}}\sum _{j=1}^{N_{2}}({\overrightarrow {X_{i}}}-{\overrightarrow {Y_{j}}})^{2}}{N_{1}\cdot N_{2}}}}={\sqrt {\frac {N_{1}\cdot SS_{2}+N_{2}\cdot SS_{1}-2\cdot {\overrightarrow {LS_{1}}}\cdot {\overrightarrow {LS_{2}}}}{N_{1}\cdot N_{2}}}}}


In multidimensional cases the square root should be replaced with a suitable norm.

Notes[edit]


^ a b Zhang, T.; Ramakrishnan, R.; Livny, M. (1996). "BIRCH: an efficient data clustering method for very large databases". Proceedings of the 1996 ACM SIGMOD international conference on Management of data  - SIGMOD '96. pp. 103–114. doi:10.1145/233269.233324.

^ "2006 SIGMOD Test of Time Award". Archived from the original on 2010-05-23.






Retrieved from "https://en.wikipedia.org/w/index.php?title=BIRCH&oldid=948081365"
Categories: Cluster analysis algorithmsHidden categories: Wikipedia articles needing clarification from December 2014






Navigation menu




Personal tools




Not logged inTalkContributionsCreate accountLog in






Namespaces




ArticleTalk






Variants












Views




ReadEditView history






More









Search



















Navigation




Main pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonateWikipedia store





Contribute




HelpCommunity portalRecent changesUpload file





Tools




What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageWikidata item





Print/export




Download as PDFPrintable version





Languages




DeutschРусский中文
Edit links






 This page was last edited on 30 March 2020, at 01:39 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Mobile view
Developers
Statistics
Cookie statement










