



Gated recurrent unit - Wikipedia





























Gated recurrent unit

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search
Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al.[1] The GRU is like a long short-term memory (LSTM) with a forget gate[2] but has fewer parameters than LSTM, as it lacks an output gate.[3] 
GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM [4]
[5]
GRUs have been shown to exhibit even better performance on certain smaller and less frequent datasets. [6][7]
However, as shown by Gail Weiss, Yoav Goldberg and Eran Yahav, the LSTM  is "strictly stronger" than the GRU as it can  easily perform unbounded counting, while the GRU cannot. That's why the GRU fails to learn simple languages that are learnable by the LSTM.[8]
Similarly, as shown by Denny Britz,  Anna Goldie,  Minh-Thang Luong and Quoc Le of Google Brain, LSTM cells consistently outperform GRU cells in "the first large-scale analysis of architecture variations for Neural Machine Translation."[9]

Contents

1 Architecture

1.1 Fully gated unit
1.2 Minimal gated unit


2 References


Architecture[edit]
There are several variations on the full gated unit, with gating done using the previous hidden state and the bias in various combinations, and a simplified form called minimal gated unit.
[10]

The operator 



⊙


{\displaystyle \odot }

 denotes the Hadamard product in the following.

Fully gated unit[edit]
 Gated Recurrent Unit, fully gated version
Initially, for 



t
=
0


{\displaystyle t=0}

, the output vector is 




h

0


=
0


{\displaystyle h_{0}=0}

. 










z

t





=

σ

g


(

W

z



x

t


+

U

z



h

t
−
1


+

b

z


)





r

t





=

σ

g


(

W

r



x

t


+

U

r



h

t
−
1


+

b

r


)








h
^




t





=

ϕ

h


(

W

h



x

t


+

U

h


(

r

t


⊙

h

t
−
1


)
+

b

h


)





h

t





=
(
1
−

z

t


)
⊙

h

t
−
1


+

z

t


⊙




h
^




t








{\displaystyle {\begin{aligned}z_{t}&=\sigma _{g}(W_{z}x_{t}+U_{z}h_{t-1}+b_{z})\\r_{t}&=\sigma _{g}(W_{r}x_{t}+U_{r}h_{t-1}+b_{r})\\{\hat {h}}_{t}&=\phi _{h}(W_{h}x_{t}+U_{h}(r_{t}\odot h_{t-1})+b_{h})\\h_{t}&=(1-z_{t})\odot h_{t-1}+z_{t}\odot {\hat {h}}_{t}\end{aligned}}}


Variables






x

t




{\displaystyle x_{t}}

: input vector





h

t




{\displaystyle h_{t}}

: output vector








h
^




t




{\displaystyle {\hat {h}}_{t}}

: candidate activation vector





z

t




{\displaystyle z_{t}}

: update gate vector





r

t




{\displaystyle r_{t}}

: reset gate vector




W


{\displaystyle W}

, 



U


{\displaystyle U}

 and 



b


{\displaystyle b}

: parameter matrices and vector
Activation functions






σ

g




{\displaystyle \sigma _{g}}

: The original is a sigmoid function.





ϕ

h




{\displaystyle \phi _{h}}

: The original is a hyperbolic tangent.
Alternative activation functions are possible, provided that 




σ

g


(
x
)
∈
[
0
,
1
]


{\displaystyle \sigma _{g}(x)\in [0,1]}

.

 Type 1
 Type 2
 Type 3
Alternate forms can be created by changing 




z

t




{\displaystyle z_{t}}

 and 




r

t




{\displaystyle r_{t}}

 [11]

Type 1, each gate depends only on the previous hidden state and the bias.









z

t





=

σ

g


(

U

z



h

t
−
1


+

b

z


)





r

t





=

σ

g


(

U

r



h

t
−
1


+

b

r


)






{\displaystyle {\begin{aligned}z_{t}&=\sigma _{g}(U_{z}h_{t-1}+b_{z})\\r_{t}&=\sigma _{g}(U_{r}h_{t-1}+b_{r})\\\end{aligned}}}


Type 2, each gate depends only on  the previous hidden state.









z

t





=

σ

g


(

U

z



h

t
−
1


)





r

t





=

σ

g


(

U

r



h

t
−
1


)






{\displaystyle {\begin{aligned}z_{t}&=\sigma _{g}(U_{z}h_{t-1})\\r_{t}&=\sigma _{g}(U_{r}h_{t-1})\\\end{aligned}}}


Type 3, each gate is computed using only the bias.









z

t





=

σ

g


(

b

z


)





r

t





=

σ

g


(

b

r


)






{\displaystyle {\begin{aligned}z_{t}&=\sigma _{g}(b_{z})\\r_{t}&=\sigma _{g}(b_{r})\\\end{aligned}}}


Minimal gated unit[edit]
The minimal gated unit is similar to the fully gated unit, except the update and reset gate vector is merged into a forget gate. This also implies that the equation for the output vector must be changed [12]










f

t





=

σ

g


(

W

f



x

t


+

U

f



h

t
−
1


+

b

f


)








h
^




t





=

ϕ

h


(

W

h



x

t


+

U

h


(

f

t


⊙

h

t
−
1


)
+

b

h


)





h

t





=
(
1
−

f

t


)
⊙

h

t
−
1


+

f

t


⊙




h
^




t








{\displaystyle {\begin{aligned}f_{t}&=\sigma _{g}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})\\{\hat {h}}_{t}&=\phi _{h}(W_{h}x_{t}+U_{h}(f_{t}\odot h_{t-1})+b_{h})\\h_{t}&=(1-f_{t})\odot h_{t-1}+f_{t}\odot {\hat {h}}_{t}\end{aligned}}}


Variables






x

t




{\displaystyle x_{t}}

: input vector





h

t




{\displaystyle h_{t}}

: output vector








h
^




t




{\displaystyle {\hat {h}}_{t}}

: candidate activation vector





f

t




{\displaystyle f_{t}}

: forget vector




W


{\displaystyle W}

, 



U


{\displaystyle U}

 and 



b


{\displaystyle b}

: parameter matrices and vector
References[edit]


^ Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (2014). "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation". arXiv:1406.1078 [cs.CL].

^ Felix Gers; Jürgen Schmidhuber; Fred Cummins (1999). "Learning to Forget: Continual Prediction with LSTM". Proc. ICANN'99, IEE, London. 1999: 850–855. doi:10.1049/cp:19991218. ISBN 0-85296-721-7.

^ "Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano – WildML". Wildml.com. 2015-10-27. Retrieved May 18, 2016.

^ Ravanelli, Mirco; Brakel, Philemon; Omologo, Maurizio; Bengio, Yoshua (2018). "Light Gated Recurrent Units for Speech Recognition". arXiv:1803.10225.

^ Su, Yuahang; Kuo, Jay (2019). "On Extended Long Short-term Memory and Dependent Bidirectional Recurrent Neural Network". arXiv:1803.01686.

^ Su, Yuanhang; Kuo, Jay (2014). "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling". arXiv:1412.3555 [cs.NE].

^  Gruber, N.; Jockisch, A. (2020), "Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?", Frontiers in Artificial Intelligence, doi:10.3389/frai.2020.00040

^ Weiss, Gail; Goldberg, Yoav; Yahav, Eran (2018). "On the Practical Computational Power of Finite Precision RNNs for Language Recognition". arXiv:1805.04908 [cs.NE].

^ Britz, Denny; Goldie, Anna; Luong, Minh-Thang; Le, Quoc (2018). "Massive Exploration of Neural Machine Translation Architectures". arXiv:1703.03906 [cs.NE].

^ Chung, Junyoung; Gulcehre, Caglar; Cho, KyungHyun; Bengio, Yoshua (2014). "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling". arXiv:1412.3555 [cs.NE].

^ Dey, Rahul; Salem, Fathi M. (2017-01-20). "Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks". arXiv:1701.05923 [cs.NE].

^ Heck, Joel; Salem, Fathi M. (2017-01-12). "Simplified Minimal Gated Unit Variations for Recurrent Neural Networks". arXiv:1701.03452 [cs.NE].






Retrieved from "https://en.wikipedia.org/w/index.php?title=Gated_recurrent_unit&oldid=968901084"
Categories: Artificial neural networks






Navigation menu




Personal tools




Not logged inTalkContributionsCreate accountLog in






Namespaces




ArticleTalk






Variants












Views




ReadEditView history






More









Search



















Navigation




Main pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonateWikipedia store





Contribute




HelpCommunity portalRecent changesUpload file





Tools




What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageWikidata item





Print/export




Download as PDFPrintable version





Languages




日本語РусскийУкраїнська粵語
Edit links






 This page was last edited on 22 July 2020, at 06:17 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Mobile view
Developers
Statistics
Cookie statement










