



Data scraping - Wikipedia





























Data scraping

From Wikipedia, the free encyclopedia



Jump to navigation
Jump to search
Data extraction technique
This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: "Data scraping" – news · newspapers · books · scholar · JSTOR (February 2011) (Learn how and when to remove this template message)
Data scraping is a technique in which a computer program extracts data from human-readable output coming from another program.

This article is part of a series onInformation security
Related security categories
Automotive security
Cybercrime
Cybersex trafficking
Computer fraud
Cyberwarfare
Internet security
Mobile security
Network security

Threats
Advanced persistent threat
Backdoors
Bootkits
Computer crime
Viruses
Denial of service
Eavesdropping
Exploits
Keyloggers
Logic bombs
Malware
Payloads
Phishing
Ransomware
Rootkits
Screen scrapers
Spyware
Trojans
Vulnerabilities
Web shells
Web application security
Worms

Defenses
Computer access control
Application security
Antivirus software
Secure coding
Secure by default
Secure by design
Secure operating systems
Authentication
Multi-factor authentication
Authorization
Data-centric security
Encryption
Firewall
Intrusion detection system
Mobile secure gateway
Runtime application self-protection (RASP)
vte
Contents

1 Description
2 Technical variants

2.1 Screen scraping
2.2 Web scraping
2.3 Report mining


3 See also
4 References
5 Further reading


Description[edit]
Normally, data transfer between programs is accomplished using data structures suited for automated processing by computers, not people.  Such interchange formats and protocols are typically rigidly structured, well-documented, easily parsed, and keep ambiguity to a minimum.  Very often, these transmissions are not human-readable at all.
Thus, the key element that distinguishes data scraping from regular parsing is that the output being scraped is intended for display to an end-user, rather than as input to another program, and is therefore usually neither documented nor structured for convenient parsing.  Data scraping often involves ignoring binary data (usually images or multimedia data), display formatting, redundant labels, superfluous commentary, and other information which is either irrelevant or hinders automated processing.
Data scraping is most often done either to interface to a legacy system, which has no other mechanism which is compatible with current hardware, or to interface to a third-party system which does not provide a more convenient API.  In the second case, the operator of the third-party system will often see screen scraping as unwanted, due to reasons such as increased system load, the loss of advertisement revenue, or the loss of control of the information content.
Data scraping is generally considered an ad hoc, inelegant technique, often used only as a "last resort" when no other mechanism for data interchange is available.  Aside from the higher programming and processing overhead, output displays intended for human consumption often change structure frequently.  Humans can cope with this easily, but a computer program may report nonsense, having been told to read data in a particular format or place and with no knowledge of how to check its results for validity.

Technical variants[edit]
Screen scraping[edit]
 A screen fragment and a screen-scraping interface (blue box with red arrow) to customize data capture process.
Although the use of physical "dumb terminal" IBM 3270s is slowly diminishing, as more and more mainframe applications acquire Web interfaces, some Web applications merely continue to use the technique of "screen scraping" to capture old screens and transfer the data to modern front-ends.[1]
Screen scraping is normally associated with the programmatic collection of visual data from a source, instead of parsing data as in Web scraping. Originally, screen scraping referred to the practice of reading text data from a computer display terminal's screen. This was generally done by reading the terminal's memory through its auxiliary port, or by connecting the terminal output port of one computer system to an input port on another. The term screen scraping is also commonly used to refer to the bidirectional exchange of data. This could be the simple cases where the controlling program navigates through the user interface, or more complex scenarios where the controlling program is entering data into an interface meant to be used by a human.
As a concrete example of a classic screen scraper, consider a hypothetical legacy system dating from the 1960s—the dawn of computerized data processing. Computer to user interfaces from that era were often simply text-based dumb terminals which were not much more than virtual teleprinters (such systems are still in use today[update], for various reasons). The desire to interface such a system to more modern systems is common.  A robust solution will often require things no longer available, such as source code, system documentation, APIs, or programmers with experience in a 50-year-old computer system.  In such cases, the only feasible solution may be to write a screen scraper that "pretends" to be a user at a terminal.  The screen scraper might connect to the legacy system via Telnet, emulate the keystrokes needed to navigate the old user interface, process the resulting display output, extract the desired data, and pass it on to the modern system. A sophisticated and resilient implementation of this kind, built on a platform providing the governance and control required by a major enterprise—e.g. change control, security, user management, data protection, operational audit, load balancing, and queue management, etc.—could be said to be an example of robotic process automation software, called RPA or RPAAI for self-guided RPA 2.0 based on artificial intelligence.
In the 1980s, financial data providers such as Reuters, Telerate, and Quotron displayed data in 24×80 format intended for a human reader. Users of this data, particularly investment banks, wrote applications to capture and convert this character data as numeric data for inclusion into calculations for trading decisions without re-keying the data. The common term for this practice, especially in the United Kingdom, was page shredding, since the results could be imagined to have passed through a paper shredder. Internally Reuters used the term 'logicized' for this conversion process, running a sophisticated computer system on VAX/VMS called the Logicizer.[2]
More modern screen scraping techniques include capturing the bitmap data from the screen and running it through an OCR engine, or for some specialised automated testing systems, matching the screen's bitmap data against expected results.[3] This can be combined in the case of GUI applications, with querying the graphical controls by programmatically obtaining references to their underlying programming objects. A sequence of screens is automatically captured and converted into a database.
Another modern adaptation to these techniques is to use, instead of a sequence of screens as input, a set of images or PDF files, so there are some overlaps with generic "document scraping" and report mining techniques.
There are many tools that can be used for screen scraping.[4]

Web scraping[edit]
Main article: Web scraping
Web pages are built using text-based mark-up languages (HTML and XHTML), and frequently contain a wealth of useful data in text form.  However, most web pages are designed for human end-users and not for ease of automated use. Because of this, tool kits that scrape web content were created. A web scraper is an API or tool to extract data from a web site. Companies like Amazon AWS and Google provide web scraping tools, services, and public data available free of cost to end-users.
Newer forms of web scraping involve listening to data feeds from web servers.  For example, JSON is commonly used as a transport storage mechanism between the client and the webserver.
Recently, companies have developed web scraping systems that rely on using techniques in DOM parsing, computer vision and natural language processing to simulate the human processing that occurs when viewing a webpage to automatically extract useful information.[5][6]
Large websites usually use defensive algorithms to protect their data from web scrapers and to limit the number of requests an IP or IP network may send. This has caused an ongoing battle between website developers and scraping developers.[7]

Report mining[edit]
Report mining is the extraction of data from human-readable computer reports. Conventional data extraction requires a connection to a working source system, suitable connectivity standards or an API, and usually complex querying. By using the source system's standard reporting options, and directing the output to a spool file instead of to a printer, static reports can be generated suitable for offline analysis via report mining.[8] This approach can avoid intensive CPU usage during business hours, can minimise end-user licence costs for ERP customers, and can offer very rapid prototyping and development of custom reports. Whereas data scraping and web scraping involve interacting with dynamic output, report mining involves extracting data from files in a human-readable format, such as HTML, PDF, or text. These can be easily generated from almost any system by intercepting the data feed to a printer. This approach can provide a quick and simple route to obtaining data without needing to program an API to the source system.

See also[edit]
Comparison of feed aggregators
Data cleansing
Data munging
Importer (computing)
Information extraction
Open data
Mashup (web application hybrid)
Metadata
Web scraping
Search engine scraping
References[edit]


^ "Back in the 1990s.. 2002 ... 2016 ... still, according to Chase Bank, a major issue. Ron Lieber (May 7, 2016). "Jamie Dimon Wants to Protect You From Innovative Start-Ups". The New York Times.

^ Contributors Fret About Reuters' Plan To Switch From Monitor Network To IDN, FX Week, 02 Nov 1990

^ Yeh, Tom (2009). "Sikuli: Using GUI Screenshots for Search and Automation" (PDF). UIST.

^ "What is Screen Scraping". June 17, 2019.

^ "Diffbot aims to make it easier for apps to read Web pages the way humans do". MIT Technology Review. Retrieved 1 December 2014.

^ "This Simple Data-Scraping Tool Could Change How Apps Are Made". WIRED. Archived from the original on 11 May 2015. Retrieved 8 May 2015. 

^ ""Unusual traffic from your computer network" - Search Help". support.google.com. Retrieved 2017-04-04.

^ Scott Steinacher, "Data Pump transforms host data", InfoWorld, 30 August 1999, p55


Further reading[edit]
Hemenway, Kevin and Calishain, Tara. Spidering Hacks. Cambridge, Massachusetts: O'Reilly, 2003. ISBN 0-596-00577-6.
vteData
Analysis
Archaeology
Cleansing
Collection
Compression
Corruption
Curation
Degradation
Editing
ETL
Farming
Format management
Fusion
Integration
Integrity
Library
Loss
Management
Migration
Mining
Pre-processing
Preservation
Protection (privacy)
Recovery
Reduction
Retention
Quality
Science
Scraping
Scrubbing
Security
Stewardship
Storage
Validation
Warehouse
Wrangling/munging





Retrieved from "https://en.wikipedia.org/w/index.php?title=Data_scraping&oldid=957345820"
Categories: Data processingHidden categories: Articles with short descriptionShort description matches WikidataArticles needing additional references from February 2011All articles needing additional referencesArticles containing potentially dated statements from 2007All articles containing potentially dated statements






Navigation menu




Personal tools




Not logged inTalkContributionsCreate accountLog in






Namespaces




ArticleTalk






Variants












Views




ReadEditView history






More









Search



















Navigation




Main pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonateWikipedia store





Contribute




HelpCommunity portalRecent changesUpload file





Tools




What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageWikidata item





Print/export




Download as PDFPrintable version





Languages




العربيةEspañolفارسیFrançais한국어Nederlands日本語Norsk bokmålPolskiPortuguêsTürkçe
Edit links






 This page was last edited on 18 May 2020, at 11:23 (UTC).
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Mobile view
Developers
Statistics
Cookie statement










